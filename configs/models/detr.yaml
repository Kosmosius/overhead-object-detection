# configs/models/detr.yaml

# DETR (DEtection TRansformer) Model Configuration

model:
  # Model architecture identifier
  architecture: "DetrForObjectDetection"

  # Backbone Configuration
  backbone:
    # Whether to use the timm library for the backbone. If false, uses AutoBackbone API.
    use_timm_backbone: true

    # Name of the backbone model to use (e.g., "resnet50", "resnet101", "tf_mobilenetv3_small_075")
    name: "resnet50"

    # Whether to use pretrained weights for the backbone
    use_pretrained: true

    # Configuration for the backbone model. Required if use_timm_backbone is false.
    # Example:
    # config:
    #   name: "resnet50"
    #   pretrained: true
    config: null

    # Additional keyword arguments for the backbone when loading from a checkpoint
    # Example: {'out_indices': [0, 1, 2, 3]}
    kwargs: null

    # Whether to replace stride with dilation in the last convolutional block (DC5).
    # Only supported when use_timm_backbone is true.
    dilation: false

  # Transformer Configuration
  transformer:
    # Dimension of the Transformer layers
    d_model: 256

    # Transformer Encoder Configuration
    encoder:
      # Number of layers in the Transformer encoder
      layers: 6

      # Number of attention heads in each Transformer encoder layer
      attention_heads: 8

      # Dimension of the feed-forward network in the Transformer encoder
      ffn_dim: 2048

      # Dropout probability for the encoder
      dropout: 0.1

      # Dropout ratio for the encoder attention probabilities
      attention_dropout: 0.0

      # Dropout ratio for activations inside the encoder feed-forward layers
      activation_dropout: 0.0

      # LayerDrop probability for the Transformer encoder layers
      layerdrop: 0.0

    # Transformer Decoder Configuration
    decoder:
      # Number of layers in the Transformer decoder
      layers: 6

      # Number of attention heads in each Transformer decoder layer
      attention_heads: 8

      # Dimension of the feed-forward network in the Transformer decoder
      ffn_dim: 2048

      # Dropout probability for the decoder
      dropout: 0.1

      # Dropout ratio for the decoder attention probabilities
      attention_dropout: 0.0

      # Dropout ratio for activations inside the decoder feed-forward layers
      activation_dropout: 0.0

      # LayerDrop probability for the Transformer decoder layers
      layerdrop: 0.0

    # Position Embedding Configuration
    position_embedding:
      # Type of position embeddings to use ("sine" or "learned")
      type: "sine"

    # Activation Function in Transformer Layers
    activation_function: "relu"  # Options: "relu", "gelu", "silu", "gelu_new"

    # Initialization Parameters
    init_std: 0.02  # Standard deviation for truncated normal initializer
    init_xavier_std: 1.0  # Scaling factor for Xavier initialization gain in HM Attention map module

  # Training Configuration
  training:
    # Number of object queries. Determines the maximum number of objects DETR can detect.
    num_queries: 100

    # Whether to use auxiliary decoding losses (loss at each decoder layer)
    # Helps the model output the correct number of objects during training
    auxiliary_loss: false

    # Loss Coefficients Configuration
    loss_coefficients:
      # Relative weight of the classification error in the Hungarian matching cost
      class_cost: 1.0

      # Relative weight of the L1 error of the bounding box coordinates in the Hungarian matching cost
      bbox_cost: 5.0

      # Relative weight of the generalized IoU loss of the bounding box in the Hungarian matching cost
      giou_cost: 2.0

      # Relative weight of the Focal loss in the panoptic segmentation loss
      mask_loss_coefficient: 1.0

      # Relative weight of the DICE/F-1 loss in the panoptic segmentation loss
      dice_loss_coefficient: 1.0

      # Relative weight of the L1 bounding box loss in the object detection loss
      bbox_loss_coefficient: 5.0

      # Relative weight of the generalized IoU loss in the object detection loss
      giou_loss_coefficient: 2.0

      # Relative classification weight of the 'no-object' class in the object detection loss
      eos_coefficient: 0.1
