# configs/models/pvtv2.yaml

pvtv2:
  # Global Configuration Parameters
  image_size: 224                            # Input image size (height, width). Can be an integer or a tuple.
  num_channels: 3                            # Number of input channels (e.g., 3 for RGB images).
  num_encoder_blocks: 4                      # Number of encoder blocks (stages) in the PVTv2 model.
  hidden_act: "gelu"                          # Activation function used in the encoder and pooler.
  hidden_dropout_prob: 0.0                    # Dropout probability for fully connected layers in embeddings, encoder, and pooler.
  attention_probs_dropout_prob: 0.0           # Dropout ratio for the attention probabilities.
  initializer_range: 0.02                     # Standard deviation for initializing weight matrices.
  drop_path_rate: 0.0                         # Dropout probability for stochastic depth in the encoder blocks.
  layer_norm_eps: 1e-6                        # Epsilon value for layer normalization.
  qkv_bias: true                              # Whether to add a learnable bias to the queries, keys, and values.
  linear_attention: false                     # Whether to use linear attention for reduced complexity.
  out_features:                               # List of feature map names to output from the backbone.
    - stage1
    - stage2
    - stage3
    - stage4
  out_indices:                                 # Indices of the encoder blocks corresponding to the output features.
    - 1
    - 2
    - 3
    - 4

  # Model-Specific Configurations
  models:
    b0:
      model_name: "OpenGVLab/pvt_v2_b0"        # Hugging Face model identifier for PVTv2-B0.
      depths: [2, 2, 2, 2]                     # Number of layers in each encoder block.
      sr_ratios: [8, 4, 2, 1]                  # Spatial reduction ratios for each encoder block.
      hidden_sizes: [32, 64, 160, 256]         # Hidden dimensions for each encoder block.
      patch_sizes: [7, 3, 3, 3]                # Patch sizes for overlapping patch embeddings.
      strides: [4, 2, 2, 2]                     # Strides for overlapping patch embeddings.
      num_attention_heads: [1, 2, 5, 8]         # Number of attention heads in each encoder block.
      mlp_ratios: [8, 8, 4, 4]                  # MLP ratios for the feed-forward networks in each encoder block.

    b1:
      model_name: "OpenGVLab/pvt_v2_b1"
      depths: [2, 2, 6, 2]
      sr_ratios: [8, 4, 2, 1]
      hidden_sizes: [64, 128, 320, 512]
      patch_sizes: [7, 3, 3, 3]
      strides: [4, 2, 2, 2]
      num_attention_heads: [2, 4, 10, 16]
      mlp_ratios: [8, 8, 4, 4]

    b2:
      model_name: "OpenGVLab/pvt_v2_b2"
      depths: [3, 4, 6, 3]
      sr_ratios: [8, 4, 2, 1]
      hidden_sizes: [64, 128, 320, 512]
      patch_sizes: [7, 3, 3, 3]
      strides: [4, 2, 2, 2]
      num_attention_heads: [2, 4, 10, 16]
      mlp_ratios: [8, 8, 4, 4]

    b3:
      model_name: "OpenGVLab/pvt_v2_b3"
      depths: [3, 4, 18, 3]
      sr_ratios: [8, 4, 2, 1]
      hidden_sizes: [96, 192, 384, 768]
      patch_sizes: [7, 3, 3, 3]
      strides: [4, 2, 2, 2]
      num_attention_heads: [3, 6, 15, 30]
      mlp_ratios: [8, 8, 4, 4]

    b4:
      model_name: "OpenGVLab/pvt_v2_b4"
      depths: [3, 8, 27, 3]
      sr_ratios: [8, 4, 2, 1]
      hidden_sizes: [128, 256, 512, 1024]
      patch_sizes: [7, 3, 3, 3]
      strides: [4, 2, 2, 2]
      num_attention_heads: [4, 8, 20, 40]
      mlp_ratios: [8, 8, 4, 4]

    b5:
      model_name: "OpenGVLab/pvt_v2_b5"
      depths: [3, 8, 27, 3]
      sr_ratios: [8, 4, 2, 1]
      hidden_sizes: [192, 384, 768, 1536]
      patch_sizes: [7, 3, 3, 3]
      strides: [4, 2, 2, 2]
      num_attention_heads: [6, 12, 30, 60]
      mlp_ratios: [8, 8, 4, 4]
